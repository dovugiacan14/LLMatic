# LLMatic with Training-Free Evaluation
In this repository, we focus on discovering the optimal neural network architecture for training on the CIFAR-10 dataset by leveraging the advanced code-generation capabilities of Large Language Models (LLMs). By utilizing evolutionary algorithms, we aim to iteratively improve the code generated by LLMs, striving to identify the best-performing network architecture for the target dataset.

Our approach combines the creativity and flexibility of LLMs with a systematic refinement process, ensuring continuous enhancement of the generated code snippets. This allows for an efficient exploration of diverse architectural designs while maintaining a focus on optimization and performance.

For reference, you can explore several code generation LLMs provided by Salesforce at: https://huggingface.co/Salesforce?search_models=codegen&sort_models=downloads#models

## Requirements
 - Python 3.9+
 - pip 24.0

## Installation

To setup environment and install the project dependencies run:

1. Create a virtual environment:
   ```sh
   python -m venv .venv
   ```

2. Activate the virtual enviroment:
   On Windows:
   ```sh
   .\.venv\Scripts\activate
   ```

3. Upgrade pip to the latest version
   To update pip to the newest version, use the following command:
   ```bash
   python -m pip install --upgrade pip
   ```

4. Check the pip version:
   ```bash
   pip --version
   ```

5. Check the Python version:
   ```bash
   python --version
   ```

6. Install the required packages from `requirements.txt`:
   ```bash
   pip install -r requirements.txt
   ```

## Usage

Run the script with the following command:

```bash
python main.py
```

## Enviroment Variables
You can adjust the environment variables in the .env file. The information includes:

- **INIT_NET_PATH**: The directory path containing the initialization network code.

- **MODEL_SUFFIX**: The suffix of Saleforces model path.

- **MAX_LENGTH**: 


